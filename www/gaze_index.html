<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Human Gaze Behavior During Visual Crowd Counting</title>
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <style>
                body {
            font: 14px "Open Sans", "Arial", sans-serif;
        }
        
        video {
            margin-top: 2px;
            border: 1px solid black;
        }
        
        .button {
            cursor: pointer;
            display: block;
            width: 160px;
            border: 1px solid black;
            font-size: 16px;
            text-align: center;
            padding-top: 2px;
            padding-bottom: 4px;
            color: white;
            background-color: darkgreen;
            text-decoration: none;
        }
  
    </style>
    <!-- <script src="./node_modules/jquery/dist/jquery.min.js"></script>
    <script src="./node_modules/sweetalert/dist/sweetalert.min.js"></script> -->

    <!-- These three scripts needed for calibration -->

    <script src="./webgazer.js"></script>
    <script src="./js/main.js"></script>
    <script src="./js/calibration.js"></script>

    <!-- <script src="./js/precision_calculation.js"></script> -->
    <!-- <script src="./js/precision_store_points.js"></script> -->
    
    <!-- Record RTC Libraries -->
    <script src="RecordRTC.js"></script>
    <script src="https://www.webrtc-experiment.com/common.js"></script>
     <!-- web streams API polyfill to support Firefox -->
     <script src="https://unpkg.com/@mattiasbuelens/web-streams-polyfill/dist/polyfill.min.js"></script>

     <!-- ../libs/DBML.js to fix video seeking issues -->
     <script src="https://www.webrtc-experiment.com/EBML.js"></script>
 
     <!-- for Edge/FF/Chrome/Opera/etc. getUserMedia support -->
     <script src="https://webrtc.github.io/adapter/adapter-latest.js"></script>
     <script src="https://www.webrtc-experiment.com/DetectRTC.js"> </script>
 
     <!-- video element -->
     <link href="https://www.webrtc-experiment.com/getHTMLMediaElement.css" rel="stylesheet">
     <script src="https://www.webrtc-experiment.com/getHTMLMediaElement.js"></script>
    
    <script>

    var globalVariable = {};
    navigator.mediaDevices.getUserMedia({
        video: true
    }).then(async function(stream) {
        let recorder = RecordRTC(stream, {
            type: 'video'
        });
        recorder.startRecording();
        recorder.camera = stream;
        globalVariable.videoRecorder = recorder;
    });
    var chunks = [];
    navigator.mediaDevices.getDisplayMedia({
        video: {
            mediaSource: 'screen',
            // displaySurface: 'window', // monitor, window, application, browser
            logicalSurface: true,
            cursor: 'never' // never, always, motion
        }
    }).then(async function(stream) {
        // let recorder = RecordRTC(stream, {
        //     type: 'video'
        // });
        // recorder.startRecording();
        // recorder.screen = stream;
        // globalVariable.screenRecorder = recorder;
        var options = {mimeType: 'video/webm; codecs=vp9'};
        var mediaRecorder = new MediaRecorder(stream, options);
        globalVariable.mediaRecorder = mediaRecorder;
        globalVariable.screen = stream;
        mediaRecorder.start();
        mediaRecorder.ondataavailable = function(e) {
            chunks.push(e.data);
        }
        mediaRecorder.onstop = function(e) {
            var blob = new Blob(chunks, { 'type' : 'video/webm; codecs=vp9'});
            // invokeSaveAsDialog(blob, "ScreenRecording"+Date.now()+".webm");
            var url = URL.createObjectURL(blob);
            var a = document.createElement('a');
            document.body.appendChild(a);
            a.style = 'display: none';
            a.href = url;
            a.download = "ScreenRecording"+Date.now()+".webm";
            a.click();
            window.URL.revokeObjectURL(url);
        }
    });
    

    function stopRecording(){
        //stopping video recording
        let videoRecorder =  globalVariable.videoRecorder;
        videoRecorder.stopRecording(function() {
            let blob = videoRecorder.getBlob();
            invokeSaveAsDialog(blob, "VideoRecording"+Date.now()+".webm");
        });
        videoRecorder.camera.stop();
        // videoRecorder.destroy();
        // videoRecorder = null;
        
        //stopping screen recording
        
        // let screenRecorder =  globalVariable.screenRecorder;
        // screenRecorder.screen.stop();
        // screenRecorder.stopRecording(function() {
        //     let blob = screenRecorder.getBlob();
        //     invokeSaveAsDialog(blob, "ScreenRecording"+Date.now()+".webm");
        // }).then(function(){
        //     screenRecorder.destroy();
        //     screenRecorder = null;
        // });

        let mediaRecorder =  globalVariable.mediaRecorder;
        mediaRecorder.stop();
        globalVariable.screen.stop();
    }
    //      console.log("The following error occurred: " + err.name);
    //   }

   
    </script>

</head>
<body data-gr-c-s-loaded="true" cz-shortcut-listen="true" >
    <!-- <video style="float: right;" height="400" width="400"></video> -->

     
<table width="1000" align="center">
    
    <tbody>
    
    <tr>
        <td width="908">
            <p align="center"><img src="./images/im11.jpeg" alt="" width="1000" height="200"> 
            </p>

            <h1 align="center"><font size="6.5" ,="" font="font" face="Arial"><strong> A Study of Human Gaze Behavior During Visual Crowd Counting </strong></font></h1>
            <p align="center"> <font size="4" ,="" font="font" face="Arial">
                Raji Annadi<sup>1</sup>, Yupei Chen<sup>2</sup>, Viresh Ranjan<sup>1</sup>, Dimitris Samaras<sup>1</sup>,  Gregory Zelinsky<sup>2</sup>, <a href="https://www3.cs.stonybrook.edu/~minhhoai/index.html" target="_blank">Minh Hoai</a><sup>1</sup>
        </font> </p>
        <p align="center">
        <font size="4" ,="" font="font" face="Arial"> <sup>1</sup>Department of Computer Science, <sup> 2</sup>Department of Psychology </font> 
        </p>
        <p align="center">
        <font size="4" ,="" font="font" face="Arial"> Stony Brook University, Stony Brook, NY 11790 </font> 
        </p>
<!--        <p align="center"> <img src="./hand_results/result_1.jpg" alt="" width="500"></p>
        <p align="center">
        <font size="3" ,="" font="font" face="Arial">Figure 1: We propose Hand-CNN, a novel network for detecting hand masks and estimating hand orientations in unconstrained conditions. </font></p> -->
        <br>
        <br>
        <p align="center">
        <font size="5" ,="" font="font" face="Arial"><b>Abstract </b></font></p>
        <p align="justify">
        <font size="4" ,="" font="font" face="Arial">In this paper, we describe our study on how humans allocate their attention during visual crowdcounting. Using an eye tracker, we collect gaze behavior of human participants who are tasked withcounting the number of people in crowd images. Analyzing the collected gaze behavior of ten humanparticipants on thirty crowd images, we observe some common approaches for visual counting. Foran image of a small crowd, the approach is to enumerate over all people or groups of people in thecrowd, and this explains the high level of similarity between the fixation density maps of different humanparticipants. For an image of a large crowd, our participants tend to focus on one section of the image,count the number of people in that section, and then extrapolate to the other sections. In terms of countaccuracy, our human participants are not as good at the counting task, compared to the performanceof the current state-of-the-art computer algorithms. Interestingly, there is a tendency to under countthe number of people in all crowd images. </font>
        
        </p>
        <p align="center">
            <font size="5" ,="" font="font" face="Arial"><b>Datasets </b></font></p> 
            
            <p align="justify">
            <font size="4" ,="" font="font" face="Arial"> This dataset contains human gaze data of 10 subjects performing visual crowd counting tasks on 30 images. All the data and code can be download from  <a href="" target="_blank" download="">here</a>. Please refer to the Readme file for the data structure.</font> </p>
            <br>
    
            <p align="center"><font size="4" ,="" font="font" face="Arial">
        <font size="5" ,="" font="font" face="Arial"><b>Paper </b></font></font></p><font size="4" ,="" font="font" face="Arial"> 
            <p align="justify">
            <font size="4" ,="" font="font" face="Arial"> <a href="https://arxiv.org/pdf/2009.06502.pdf" target="_blank">A Study of Human Gaze Behavior During Visual Crowd Counting</a>. Raji Annadi, Yupei Chen, Viresh Ranjan, Dimitris Samaras, Gregory Zelinsky, Minh Hoai.
                <br>
                <br>
                If you find this work useful in your research please cite our work using this <a href="./HumanGazeBehaviour_bibtext.bib" target="_blank" download="">BibTeX</a>. </font>
            <br>
        <p align="center">
        <font size="5" ,="" font="font" face="Arial"><b>Examples </b></font> 
        </p>
        <p align="center">
            <img src="./images/img_0189.png" alt="" width="400" height="300"/>
            <img src="./images/fix_map.jpg" alt="" width="400" height="300"/> 
        
            
            <img src="./images/im07.jpg" alt="" width="400" height="300"/>
            <img src="./images/fix_sp2.jpg"  width="400" height="300"/> 
        </p>
        <p align="center">
        <font size="3" ,="" font="font" face="Arial">Figure 1: Some sparse crowd images and their fixation density maps. </font></p> 


        <p align="center">
            <img src="./images/img_0166.png" alt="" width="400" height="300"/>
            <img src="./images/fix_map_dense.jpg" alt="" width="400" height="300"/> 
        
            
            <img src="./images/im04.jpg" alt="" width="400" height="300"/>
            <img src="./images/Fixmap_s02_ Trial_ 24.png"  width="400" height="300"/> 
        </p>
        <p align="center">
        <font size="3" ,="" font="font" face="Arial">Figure 2: Some dense crowd images and their fixation density maps. </font></p> 
        
       
         <br>

        </font></p>

        <p align="center">
            <font size="5" ,="" font="font" face="Arial"><b>Copyright notice </b></font> 
            <p align="justify">
                <font size="4" ,="" font="font" face="Arial"> The documents contained in these directories are included by the contributing authors as a means to ensure timely dissemination of scholarly and technical work on a non-commercial basis. Copyright and all rights therein are maintained by the authors or by other copyright holders, notwithstanding that they have offered their works here electronically. It is understood that all persons copying this information will adhere to the terms and constraints invoked by each author's copyright. These works may not be reposted without the explicit permission of the copyright holder.</p>
                <br>
            </p>
        </p>
       
        <br>
        <br>
        <br>
        <br>
        <br>
    </p></font>
    <button id='stop' onclick="stopRecording()">stop</button>
    </td></tr>
    
    </tbody>
</table>

<span class="gr__tooltip"><span class="gr__tooltip-content"></span><i class="gr__tooltip-logo"></i><span class="gr__triangle"></span></span>
</body>